# Template for the global .env file.
# Rename this file to .env and fill in the required values.

# AWS #
AWS_ACCOUNT_ID=
AWS_PROFILE=chaturai
AWS_REGION=us-central-1
AWS_SG=
AWS_SUBNET=

# DOCKER #
DOCKER_LITELLM_IMAGE=ghcr.io/berriai/litellm:main-v1.67.0-stable
DOCKER_PG_VECTOR_IMAGE = pgvector/pgvector:pg16
DOCKER_REDIS_IMAGE=redis:6.0-alpine
DOCKER_SCRAPER_IMAGE = naps-scraper

# DOMAIN #
DOMAIN_NAME=localhost

# LITELLM #
# Notes
#
# 1. All model names come under "openai/..." and correspond to the "model_name" in the
# proxy config.yaml. "openai/..." is needed since the proxy presents a unified
# OpenAI-style API for all of its endpoints.
#
# 2. The API key is required but we just need a dummy key. The actual OPENAI_API_KEY is
# set in the LiteLLM proxy container.
LITELLM_API_KEY=dummy-key
LITELLM_ENDPOINT=http://localhost:4000
LITELLM_MODEL_CHAT=openai/chat
LITELLM_MODEL_DEFAULT=openai/default
LITELLM_MODEL_EMBEDDING=openai/embedding

# OPENAI #
OPENAI_API_KEY=sk-...

# PATHS #
# 1. Do NOT include trailing slashes in the paths.
PATHS_BACKEND_ROOT=
PATHS_LOGS_DIR=~/chaturai/logs
PATHS_PROJECT_DIR=~/chaturai
PATHS_SECRETS_DIR=~/chaturai/secrets

# REDIS #
REDIS_CACHE_PREFIX_BROWSER_STATE=browser_state
REDIS_CACHE_PREFIX_CHAT=chat_sessions
REDIS_CACHE_PREFIX_GRAPH_CHATUR=graph_chatur
REDIS_HOST=redis://localhost
REDIS_PORT=6379
